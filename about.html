<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>An Acousmatic Network for Neural Audio Synthesis</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 20px;
                text-align: center;
                background-color: #ffffff;
            }
            h1,
            h2,
            h3 {
                color: #000;
                margin: 20px 0;
            }
            p,
            table,
            pre {
                margin: 20px auto;
                max-width: 800px;
                text-align: left;
            }
            table {
                width: 100%;
                border-collapse: collapse;
                margin-bottom: 20px;
                font-family: Arial, sans-serif;
                font-size: 14px;
            }
            th,
            td {
                border: 1px solid #dddddd;
                padding: 8px;
                text-align: left;
            }
            th {
                background-color: #f2f2f2;
                font-weight: bold;
            }
            td {
                white-space: nowrap;
            }

            .figure-container {
                display: flex;
                flex-direction: column;
                align-items: center;
            }

            .figure-container figure {
                width: 100%; /* Set your desired width */
                max-width: 1000px; /* Optional max-width */
                margin: 0 auto; /* Center the figure */
                text-align: center;
            }

            .figure-container img {
                width: 100%;
                height: auto; /* Maintain aspect ratio */
            }

            .figure-container figcaption {
                margin-top: 10px;
                font-size: 1em;
                color: #555;
            }

            figure {
                margin: 20px auto;
                max-width: 800px;
                text-align: center;
            }
            figcaption {
                margin-top: 10px;
            }
            .grid {
                display: grid;
                gap: 10px;
                justify-content: center;
                margin: 20px auto;
            }
            .grid-4x2 {
                grid-template-columns: repeat(2, 1fr);
            }
            .grid-4x3 {
                grid-template-columns: repeat(3, 1fr);
            }
            .grid-3x3 {
                grid-template-columns: repeat(3, 1fr);
            }
            .grid-2x3 {
                grid-template-columns: repeat(2, 1fr);
            }
            audio {
                display: block;
                margin: 0 auto;
            }
            code {
                display: block;
                background: #e0e0e0;
                padding: 10px;
                border: 1px solid #000;
            }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script
            id="MathJax-script"
            async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
        ></script>
    </head>
    <body>
        <h1>An Acousmatic Approach to Neural Audio Synthesis</h1>
        <h4>
            <i
                >Companion site to the thesis
                <a href="./index.html"
                    >An Acousmatic Approach to Neural Audio Synthesis</a
                >
                (Ardito, 2024)</i
            >
        </h4>
        <div style="text-align: center">
            <figure>
                <img
                    src="top-group.png"
                    alt="Topological mapping diagram"
                    style="width: 60%"
                />
                <figcaption>
                    <i
                        >A topological interpretation of the DDSP harmonic autoencoder
                        from [EHGR20]</i
                    >
                </figcaption>
            </figure>
        </div>
        <h2>Introduction</h2>
        <p>
            It can be argued that many contemporary advancements in audio signal
            processing also correlate with radical shifts in compositional form, in which
            unprecedented musical practices reveal themselves through technological
            experimentation. One primary example of this phenomenon can be found in the
            philosophy of acousmatic music—a form of electronic music composition that
            emerged in mid-20th century France, where sound is experienced without a
            visible source through spatialized loudspeakers, recorded sound manipulation,
            and analog sound synthesis. The practice of acousmatic music evolved alongside
            advancements in engineering and signal theory, allowing composers to explore
            new ways of presenting sound. Collaborations between engineers and composers,
            especially at institutions like the Groupe de Recherches Musicales (GRM), led
            to radical reformulations of both compositional techniques and philosophical
            ideas about sound. Many of these reformalizations can be found in the writings
            of the GRM's first director Pierre Schaeffer, particularly in his
            <i>Traité des objets musicaux</i> [Sch66]. Schaeffer argued that these
            emerging audio technologies warranted an entirely new taxonomy for analyzing
            sound compositionally. This came to be known as <i>typomorphology</i>, a sort
            of music theory for the analysis of the "typology" and "morphology" of what
            Schaeffer’s denoted as the "sound object"—an invariant, theoretical unit of
            sound.
        </p>
        <p>
            Interestingly, it is seldom pointed out how easily the acousmatic philosophy
            of sound can be placed in dialogue with similar developments in 21st-century
            audio signal processing, namely the advent of data-driven audio generation and
            classification models. These models, commonly known as
            <i>neural audio models</i>, utilize deep neural networks (DNNs) as universal
            function approximators for generation and classification tasks. We can
            demonstrate this parallel by first making the observation that both acousmatic
            music and neural audio synthesis operate in a "black box" environment where
            the source of sound is hidden. Continuing this comparison then requires a more
            rigorous investigation into the epistemological roots of both Schaeffer's
            philosophy and the mathematical foundations of machine learning. When analyzed
            sufficiently, we might come to the conclusion that both of these forms borrow
            a vast body of knowledge from the geometric and mathematical theories of
            invariance. This thesis thus draws a connection between acousmatic music and
            modern machine learning by way of concepts such as geometric stability and
            group theory. We demonstrate this through [Kan14]'s argument that Schaefferian
            philosophy was primarily influenced by Husserlian phenomenology, and through
            [BBCV21]'s foundational work on geometric approaches to deep learning.
        </p>
        <p>
            Based on these texts, we can formalize this epistemic connection by modeling
            the Schaefferian sound object mathematically as a continuous manifold with Lie
            group structure. Using geometric approaches to deep learning, we can then
            review how something like the wavelet scattering interpretation of the
            Convolutional Neural Network (CNN) [Mal16] [AM14] can geometrically represent
            the sound object’s mesostructural properties [VML23]. We can then hypothesize
            how other neural audio models like Differentiable Digital Signal Processing
            (DDSP) models [EHGR20] [BRC24] can capture the microstructural properties of
            sound while preserving invariant parametric transformations. Through a series
            of experiments with synthetic sounds and recordings of friction-based
            percussion, we demonstrate a novel method for analyzing and synthesizing
            acousmatic sound objects using neural audio techniques. This method leverages
            neural audio models to disentangle key perceptual parameters unique to the
            dataset without compromising the structural integrity of the parametric
            invariance found at different time-scales. The results highlight the potential
            for using time-varying spectral audio descriptors to control DDSP models,
            marking a new approach to parametric neural audio synthesis.
        </p>

        <h2>Modeling Typomorphology using Group Representations</h2>

        <p>
            In his book <i>Sound Unseen</i>, music theorist Brian Kane points out a number
            of similarities between Schaeffer's philosophy of sound and Husserlian
            phenomenology [Kan14]. A striking comparison in itself, Kane's analysis
            becomes even more fruitful when looked at in terms of Husserl's mathematical
            influences. For instance, Husserl’s method of examining the essence of objects
            by identifying their imagined invariant properties not only reflects
            Schaeffer's writings about the invariance of the sound object, but also
            reflects an important influence from contemporaneous shifts in mathematical
            thinking, particularly in the realm of geometric group theory. Husserl's use
            of the term invariance was not accidental, but rooted in his decision to align
            his formal ontology with mathematics. Influenced by Hilbert's axiomatization
            of Euclidean geometry, Cantor's set theory, and most notably Felix Klein’s
            <i>Erlangen Programme</i>, Husserl drew from the reorientation of geometry as
            the study of abstract structures, centered around the mathematical
            <i>group</i>. This shift, from viewing geometry as the study of physical space
            to understanding it as the science of possible forms, allowed for a move to
            the study of invariant structures under transformations. Figures like Cayley,
            Sylvester, Grassmann, Riemann, and others, whose work on algebraic invariants
            and geometric transformations laid the groundwork for this shift, also
            profoundly impacted Husserl’s own development of phenomenological invariance.
            According to [Rou23], Husserl was particularly influenced by two aspects of
            these geometric ideas: the epistemic transition from experienced shapes to
            geometric shapes, and the understanding of geometric concepts through
            interaction with the physical world and imaginative variation. We can see this
            quite clearly in Schaeffer's approach to the invariance of the sound object,
            which can be interpreted as the sonic analogue to both Klein and Husserl’s
            explorations of invariant geometry, each methodology sharing a common
            historical and philosophical context. Based on this connection, an argument
            could be made for the mathematical language of the Erlangen Programme to be
            integrated into the Schaefferian project—a reinterpretation of typomorphology
            with respect to the language of group representation theory.
        </p>
        <p>
            This doesn't merely produce a sort of "historically informed" methodology for
            thinking about the theory of acousmatic music. Rather, group representation
            theory invites the structural possibility for thinking about sound as
            something entirely independent of physical geometric form. In contemporary
            audio signal processing, we often represent sound by somewhat conservatively
            associating various mathematical structures with precise physical attributes
            of acoustic systems—e.g. a continuous temporal representation \(x(t) \in
            \mathbb{R}\) or a Fourier representation \(X(t, f) \in \mathbb{C}\). While
            these models are functional, they do not reveal much about the underlying
            geometric structure of the sound we're attempting to model. Instead, we might
            talk about a sound \(x\) as existing in a space endowed with the underlying
            symmetric structure of an unknown group \(\mathfrak{G}\). We might then make
            one more important assumption necessary for doing neural audio synthesis: the
            assumption that this group is a Lie group (a differentiable manifold) which we
            denote with a subscript \(\tau\). By then defining a
            <i>group representation</i> \(\mathcal{X}\) over the space, we can do things
            like define the existence of a Haar measure \(\mu_{\mathfrak{G}}\) and a norm
            \(\lVert{\cdot\rVert}_{\mathfrak{G}}\). Notice that we have yet to impose any
            severe structural bias onto our sound's representation—in other words, we've
            derived some common metrics that allude to measurability and algebraic
            structure, yet we've kept any declarations of a tangeable geometric form
            completely abstracted. The group representation thus serves as a flexible
            basis for any number of geometric forms sound might take, shown by easily
            defining a homomorphism from the representation onto an \(L^{2}\) space
            [BBCV21], where we are more familiar with doing sound processing.
        </p>
        <center>
            \(\mathcal{X}(\mathfrak{G}_{\tau}) = \lbrace{x : \mathfrak{G}_{\tau}
            \rightarrow L^{2}(\mathfrak{G}_{\tau})\rbrace}\)
        </center>
        <p>
            Of course, the canonical paradigms mentioned above still fit quite naturally
            into this methodology. For instance, \(\mathcal{X}\) can be thought of as a
            time-frequency dictionary—a set of elementary functions \(\mathcal{X} =
            \lbrace{\chi_{u, \xi}\rbrace}_{u, \xi \in \Lambda}\) that form a group
            representation, where \(u\) and \(\xi\) are time-frequency coordinates. We can
            then talk about something like the Short-Time Fourier Transform (STFT) not
            simply as a tool for analyzing the physical world, but rather as a
            <i>representation</i> of the structural bias imposed by the Weyl-Heisenberg
            group \(\mathfrak{W}_{\tau}\). Ensuring a sort of geometric and
            phenomenological foresight, we can then familiarize ourselves with the
            representation's forms of transformational invariance by taking a sound \(x
            \in \mathcal{X}(\mathfrak{W}_{\tau})\) analying it by way of
            \(\mathfrak{W}_{\tau}\)'s group actions of translation \(\rho(\mathfrak{t})\)
            and modulation \(\rho(\mathfrak{m})\):
        </p>
        <center>\(\rho(\mathfrak{t})x = x(t - \mathfrak{t})\)</center>
        <center>\(\rho(\mathfrak{m})x = x(t)e^{i2\pi\mathfrak{m}t}\)</center>
        <p>
            This group representational approach also ends up aligning quite nicely with
            many paradigms in machine learning. For example, a deep learning model like a
            CNN can be interpreted from a computational perspective as a model that
            "learns" patterns at multiple scales in images, videos, and audio. However,
            this says more about what's happening phenomenologically rather than what's
            actually happening on a geometric level. Instead, we could say that the CNN is
            actually a function that is approximately invariant (geometrically stable) to
            affine transformations \(\rho(\mathfrak{a})\) [BSL13]. This same kind of
            invariance has since been reframed in the development of the wavelet
            scattering transform, an analogous construct to the CNN that produces a
            representation that is both translation invariant and stable to warping
            deformations across both axes of the time-frequency plane. This geometric
            stability is made possibly by the wavelet representation of sound, which
            yields an "affinized" group representation
            \(\mathcal{X}(\mathfrak{A}_{\tau})\), where \(\mathfrak{A}_{\tau}\) denotes
            the affine group. The continuous wavelet transform (CWT) then acts as an
            operator that projects \(x\) into an affine domain:
        </p>

        <center>
            \(\Phi_{x}^{\mathcal{W}}(a, b) = \frac{1}{\sqrt{a}}\int_{t \in
            \mathbb{R}}x(t)\psi^{\ast}\left(\frac{t - b}{a}\right)dt\)
        </center>

        <p>
            So what would a neural audio model like DDSP look like in terms of these
            geometric transformations? In geometric deep learning, we commonly want to
            find what's called a <i>disentangled representation</i>. Introduced by
            [HAP+18], a disentangled representation is a sort of cross-disciplinary
            generalization of the <i>Peter-Weyl Theorem</i>: a representation that
            associates the group actions of irreducible subgroups with linearly
            independent subspaces. An example of a disentangled representation can
            actually be found in the wavelet representation, whose inherent
            multiresolution decomposition produces separable subspaces.
        </p>
        <p>
            This type of disentanglement provided by the wavelet representation often
            takes place over a mesostructural time-scale [VML23]. DDSP models, however,
            work at the scale of a single STFT window \(w\) and extract a matrix of
            conditional parameters, thus working on a much smaller microstructural
            time-scale. This parameter marix is then fed through a series of neural
            networks to output another matrix of resynthesis parameters that are
            compatible with a common synthesizer such as a noise-driven filterbank or a
            harmonic-plus-noise model. We can define these operations in terms of
            disentangled subspaces (recall that \(\mathfrak{W}_{\tau}\) is the
            Weyl-Heisenberg group from before):
        </p>

        <center>
            \(\Gamma : \mathcal{X}(\mathfrak{W}_{\tau}) \rightarrow \bigoplus_{k =
            1}^{K}W_{k}(\mathfrak{A}_{\tau})\)
        </center>
        <center>
            \(\tilde{\Gamma} : \bigoplus_{l = 1}^{L}W_{L}(\mathfrak{A}_{\tau}) \rightarrow
            \mathcal{X}(\mathfrak{W}_{\tau})\)
        </center>

        <p>
            where \(K\) is the number of conditional parameters and \(L\) is the number of
            resynthesis parameters. The key assumption behind disentanglement is that each
            group action on each conditional subspace \(\rho_{|W_{k}}(\mathfrak{a})\) acts
            on the space completely independently. The implication of this statement not
            only alludes to the fact that the DDSP network learns these parametric actions
            separately, it also hints at the assumption that each parametric action of the
            model exhibits perceptually independent control of the output sound.
        </p>
        <p>
            The DDSP model itself would then be denoted as a mapping between these two
            disentangled parametric spaces \(f : \bigoplus_{k =
            1}^{K}W_{k}(\mathfrak{A}_{\tau}) \rightarrow \bigoplus_{l =
            1}^{L}W_{L}(\mathfrak{A}_{\tau})\). The evaluation of the model is often
            performed using <i>Multiscale Spectral Loss</i> (MSS) [SM23], which—much like
            the wavelet representation—projects sound into an affinized space, this time
            on the <i>microscale</i>. A complete commutative diagram of DDSP can be seen
            here:
        </p>

        <center>
            <img src="diagram.png" alt="Type A [Spring Coil]" style="width: 50%" />
        </center>

        <p>
            The right side of this diagram can be interpreted as the complete analysis and
            resynthesis of a single microsound through a given DDSP autoencoder model,
            while the left side denotes the evaluation of the reconstructed microsound
            based on its spectral loss. The perceptual deviation from the original and
            reconstructed microsounds can be generalized by the magnitude of the group
            action \(\lvert{\mathfrak{a}\rvert}\). We also define an extra operator
            \(\mathscr{T}\) for sound matching (or "timbre transfer").
        </p>

        <h2>The Disentanglement Hypothesis</h2>
        <!-- % Scientific phantasmagoria -->
        <p>
            At this point, we might return to the question: why use this new theoretical
            language to talk about sound? This geometric approach hints at something very
            important that stems from the unique position of dealing with acousmatic
            sound, be it as an engineer or as a listener. Take for instance the DDSP
            model, which uses MSS as an evaluation metric for microsounds. While this
            technique might be relevant to sound on the microscale, it says nothing about
            the invariant properties of sound on the mesoscale. In other words, parameters
            might be disentangled at the scale of an STFT window, but when a DDSP
            reconstruction is concatenated together at inference time, the mesostructural
            properties of the output sound might deviate from the expected mesostructural
            properties of the dataset. In other words, DDSP does not disentangle the
            microstructrual parameters from the mesostructural parameters of sound, since
            neither the functional nor perceptual independence of parameters at this scale
            are taken into account solely by the MSS metric. These shortcomings thus
            emphasize the fundamental concerns of the Schaefferian project: that
            acousmatic sound (sound without a sound source, sound in a dataset) requires
            an entirely new theoretical framework for analysis in order to be presented to
            the listener as invariant.
        </p>
        <p>
            A large contribution in this work thus culminates in the practical evaluation
            of a disentanglement hypothesis to DDSP's conditional parameters. The task
            here is to extract optimal microstructural control parameters entirely based
            on a dataset of sounds to effectively control and condition a DDSP model
            during training. In our experiments, we restrict our selection of control
            parameters to the time-varying spectral audio descriptors laid out in
            [PGS+11]. By interpreting the problem from a group-theoretical perspective, a
            given microsound \(x \in \mathcal{X}(\mathfrak{W}_{\tau})\) is projected via
            an operator \(\Gamma\) into \(K\) approximately independent subspaces
            \(\bigoplus_{k=1}^{K}W_{k}(\mathfrak{A}_{\tau})\). This projection ensures
            that group actions are orthogonal, with each parameter
            \(\rho|_{W_{k}}(\mathfrak{g})\) acting only on its designated subspace
            \(W_{k}\), enabling disentanglement of control parameters as described by
            [HAP+18]. This process yields a latent space where each parameter controls
            distinct, perceptually independent aspects of the output.
        </p>
        <p>
            An initial implementation of \(\Gamma\) could use Principal Component Analysis
            (PCA) to extract control parameters that capture the most variance across the
            dataset. However, PCA would focus its analysis only on microstructural
            differences between sounds, limiting its scope. A more effective approach
            would involve incorporating mesostructural representations to maintain
            equivariance between DDSP's control parameters and resynthesis parameters at
            both the micro and mesoscales. This would allow the model to generate an
            expressive latent morphology of sounds that remains invariant to larger
            timbral structures. In Schaefferian terms, this approach disentangles the
            sound’s morphology from its typology, offering a more perceptually meaningful
            representation.
        </p>
        <p>
            The method discussed in this thesis thus focuses on identifying a small set of
            spectrotemporal control parameters most correlated with the features of a
            low-dimensional projection of the dataset’s Joint Time-Frequency Scattering
            (JTFS) representation, as implemented by [LYY23]. Using the isomap algorithm,
            the dimensionality of the JTFS representation is reduced while preserving
            small Euclidean distances in feature space, ensuring that high-dimensional
            representations are mapped onto an orthogonal, low-dimensional space. This
            three-dimensional feature space correlates with perceived timbral similarity,
            as shown by [LEHR+21].
        </p>
        <p>
            This approach furthermore demonstrates how microstructural control parameters
            at the time-scale of an STFT window \(w_{T}\) must geometrically correlate
            with mesostructural representations at the larger time-scale of a low-pass
            scattering filter \(\phi_{T}\) in order for a meaningful representation of
            sound to be achieved in neural audio synthesis. Be sure to look at some
            examples and results on the
            <a href="./index.html">results</a> page, which serves as a supplement to
            results chapter of the thesis.
        </p>

        <h2>Results and Acknowledgements</h2>
        <p>
            Additionally, please consider citing the thesis if you want to reference its
            ideas or experiements:
        </p>
        <pre>
        <code>
        @mastersthesis{Ardito2024,
          author       = {Max Ardito},
          title        = {{An Acousmatic Approach to Neural Audio Synthesis}},
          school       = {McGill University},
          year         = {2024},
          address      = {Montréal, Quebec, Canada},
          type         = {Master's Thesis},
          url          = {(Thesis still under review!)},
        }
        </code>
        </pre>

        <h2>Citations</h2>
        <p>
            [BRC24] Adrián Barahona-Ríos and Tom Collins. NoiseBandNet: Controllable Time-
            Varying Neural Synthesis of Sound Effects Using Filterbanks. IEEE/ACM
            Transactions on Audio, Speech, and Language Processing, 32:1573–1585, 2024.
        </p>
        <p>
            [EHGR20] Jesse Engel, Lamtharn Hantrakul, Chenjie Gu, and Adam Roberts. DDSP:
            Differential Digital Signal Processing. CoRR, abs/2001.04643, Jan 2020. https:
            //arxiv.org/abs/2001.04643.
        </p>
        <p>
            [HAP+ 18] Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic
            Matthey, Danilo Rezende, and Alexander Lerchner. Towards a Definition of
            Disentangled Representations, 2018. https://arxiv.org/abs/1812.02230.
        </p>
        <p>
            [LEHR+ 21] Vincent Lostanlen, Chady El-Hajj, Matthieu Rossignol, Gérard Lafay,
            Joakim Andén, and Mathieu Lagrange. Time-Frequency Scattering Accurately
            Models Auditory Similarities Between Instrumental Playing Techniques. EURASIP
            Journal on Audio, Speech, and Music Processing, 2021(1):3, 2021. Epub 2021 Jan
            11.
        </p>
        <p>
            [LYY23] Vincent Lostanlen, Lingyao Yan, and Xianyi Yang. From HEAR to GEAR:
            Generative Evaluation of Audio Representations. Proceedings of Machine
            Learning Research, (166):48–64, February 2023.
        </p>
        <p>
            [PGS+ 11] Geoffroy Peeters, Bruno L. Giordano, Patrick Susini, Nicolas
            Misdariis, and Stephen McAdams. The Timbre Toolbox: Extracting Audio
            Descriptors from Musical Signals. The Journal of the Acoustical Society of
            America, 130(5):2902– 2916, Nov 2011.
        </p>
        <p>
            [Sch66] Pierre Schaeffer. Traité des objets musicaux. Éditions du Seuil, 27
            rue Jacob, Paris, 1966.
        </p>
        <p>
            [VML23] Cyrus Vahidi, Christopher Mitcheltree, and Vincent Lostanlen. ISMIR
            2023 tutorial. https://www.kymat.io/ismir23-tutorial/intro.html, 2023.
            Presented at the International Society for Music Information Retrieval (ISMIR)
            2023.
        </p>
    </body>
</html>
