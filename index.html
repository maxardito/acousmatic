<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>An Acousmatic Network for Neural Audio Synthesis</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 20px;
                text-align: center;
                background-color: #f5f5f5;
            }
            h1,
            h2,
            h3 {
                color: #000;
                margin: 20px 0;
            }
            p,
            table,
            pre {
                margin: 20px auto;
                max-width: 800px;
                text-align: left;
            }
            table {
                width: 100%;
                border-collapse: collapse;
                margin-bottom: 20px;
                font-family: Arial, sans-serif;
                font-size: 14px;
            }
            th,
            td {
                border: 1px solid #dddddd;
                padding: 8px;
                text-align: left;
            }
            th {
                background-color: #f2f2f2;
                font-weight: bold;
            }
            td {
                white-space: nowrap;
            }

            .figure-container {
                display: flex;
                flex-direction: column;
                align-items: center;
            }

            .figure-container figure {
                width: 100%; /* Set your desired width */
                max-width: 1000px; /* Optional max-width */
                margin: 0 auto; /* Center the figure */
                text-align: center;
            }

            .figure-container img {
                width: 100%;
                height: auto; /* Maintain aspect ratio */
            }

            .figure-container figcaption {
                margin-top: 10px;
                font-size: 1em;
                color: #555;
            }

            figure {
                margin: 20px auto;
                max-width: 800px;
                text-align: center;
            }
            figcaption {
                margin-top: 10px;
            }
            .grid {
                display: grid;
                gap: 10px;
                justify-content: center;
                margin: 20px auto;
            }
            .grid-4x2 {
                grid-template-columns: repeat(2, 1fr);
            }
            .grid-4x3 {
                grid-template-columns: repeat(3, 1fr);
            }
            .grid-3x3 {
                grid-template-columns: repeat(3, 1fr);
            }
            .grid-2x3 {
                grid-template-columns: repeat(2, 1fr);
            }
            audio {
                display: block;
                margin: 0 auto;
            }
            code {
                display: block;
                background: #e0e0e0;
                padding: 10px;
                border: 1px solid #000;
            }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script
            id="MathJax-script"
            async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
        ></script>
    </head>
    <body>
        <h1>An Acousmatic Approach to Neural Audio Synthesis</h1>
        <!-- <p> -->
        <!--     In recent years, neural audio synthesis has gained much attention in the field -->
        <!--     of computer music. While various literature exists relating these ideas to -->
        <!--     improving sound quality and modeling 21st century forms of synthesis, very few -->
        <!--     work has also attempted to grapple with the philosophy of sound in parallel. -->
        <!-- </p> -->

        <!-- <p> -->
        <!--     This page presents some ideas on acousmatic approaches to neural audio -->
        <!--     synthesis, casually presenting some ideas from the <i>Acousmatic Networks</i>. -->
        <!-- </p> -->

        <!-- <h2>Acousmatic Sound and The Sound Object</h2> -->
        <!-- <p> -->
        <!--     In other words, there is nothing specifically technological about the -->
        <!--     “objectivity” of the sound object. A sound object could be demonstrated any -->
        <!--     number of ways within an acousmatic setting—not only using audio technology -->
        <!--     but also through the listener’s own imagination [Kan14]. It then becomes -->
        <!--     innevitable that if the listener is barred from all tangeable physical cues -->
        <!--     that might aid in understanding the sound source, the sound object must reveal -->
        <!--     itself through its multiplicity of parametric variations, often organized -->
        <!--     formally by the composer of a piece of acousmatic music. It is about -->
        <!--     <i>invariance.</i> -->
        <!-- </p> -->

        <!-- <p> -->
        <!--     Brian Kane also argues that much of Schaeffer's philosophy of sound was -->
        <!--     influenced by phenomenologist Edmond Husserl, who often talked about the -->
        <!--     invariance of objects more generally. This can be argued to relate to a -->
        <!--     particular influence from the geometric philosophies of The Erlangen Programme -->
        <!--     represented a departure from thinking about geometry as a field concerned with -->
        <!--     the structure of physical space towards thinking about geometry as a field -->
        <!--     concerned with abstract notions of structure, primarily centered around the -->
        <!--     mathematical notion of groups, group transformations, and group invariance -->
        <!--     [Tie05] 2 . This reorientation of geometry around the notion of the group -->
        <!--     enabled geometry to “no longer ... be considered as the theory of the -->
        <!--     structure of physical space but rather as the science of possible space forms -->
        <!--     regardless of their being of physical space or of certain of its parts” -->
        <!--     [Mor91]. Geometry thus came to be the investigation of everything that is -->
        <!--     invariant under the transformations of the given group, expressed by “the -->
        <!--     axioms, definitions and theorems that are or could be set up for each -->
        <!--     particular geometry” [Tie05]. -->
        <!-- </p> -->

        <!-- <h2>Modeling The Sound Object's Morphology</h2> -->
        <!-- <p> -->
        <!--     Much like Schaeffer did with music, many authors treat the field of deep -->
        <!--     learning in a way that talks about invariance. One notable paper by Higgins et -->
        <!--     al. (...) uses the language of group representation theory to try to get at a -->
        <!--     notion of data in the same way the Schaeffer gets at the "sound itself." The -->
        <!--     hypothesis for Higgins surrounds the notion of geometric group actions that -->
        <!--     act on a representational subspace. -->
        <!-- </p> -->
        <!-- <p> -->
        <!--     the most desirable neural network architectures for representation learning -->
        <!--     are ones in which the layers disentangle input representations, since this -->
        <!--     allows for interpretability. -->
        <!-- </p> -->
        <!-- <p>\[ \rho|_{V} = \bigoplus_{l = 1}^{L}\rho|_{W_{l}} \]</p> -->
        <!-- <p> -->
        <!--     The most common model for audio synthesis is Differentiable Digital Signal -->
        <!--     Processing. The way DDSP thinks of a dataset is by finding an equivariant -->
        <!--     mapping between control parameters to affine representations of the dataset -->
        <!--     such that affine transformations are preserved. Consider our sound objects as -->
        <!--     vectors... This is done by first defining an operator \Gamma that extracts -->
        <!--     independent control parameters from the dataset and associates them with a -->
        <!--     perceptual domain on the microscale. This is done through MSS. -->
        <!-- </p> -->
        <!-- <p> -->
        <!--     However, in order to perform this, we must define a synthesizer. If we didn't, -->
        <!--     we'd have to learn the parametric space of the Weyl_Heisenberg group which is -->
        <!--     not only huge but unstable. -->
        <!-- </p> -->
        <!-- <p> -->
        <!--     Finding these "disentangled" control parameters for a model like DDSP, -->
        <!--     however, requires some care regarding the musical structure of the sound -->
        <!--     object. At first glance, we might think to derive these control parameters by -->
        <!--     taking a set of control parameters. We can consider something like the -->
        <!--     time-varying audio descriptors from Peeters et al. (...), which consist of a -->
        <!--     list of spectral audio descriptors whose ... correlate to human perception. -->
        <!--     (LIST OF AUDIO DESCRIPTORS) -->
        <!-- </p> -->
        <!-- <p> -->
        <!--     We can try to find which parameters are the least correlated in a dataset -->
        <!--     using a PCA or something. This allows us to derive a notion of approximate -->
        <!--     orthogonality, since the least correlated control parameters will -->
        <!--     approximately form the most disentangled subspaces for control. -->
        <!-- </p> -->
        <!-- <p> -->
        <!--     But while this might seem like a proper approach, it disregards important -->
        <!--     notions of musical structure. This notion of correlation accounts for -->
        <!--     expressivity on the microscale. However, sound is not just something that we -->
        <!--     perceive on a microscale, we also understand notions of timbre distributions -->
        <!--     on the <i>mesoscale</i>. We can interpret this group theoretically, showing -->
        <!--     that our model might construct an equivariant relationship between control -->
        <!--     parameters and sounds, but at this scale of control it might not be invariant -->
        <!--     larger timbral structures. To put in Schaefferian terms, the timbral model -->
        <!--     might yield an expressive <i>morphology</i> of sound, but it disregards a -->
        <!--     proper <i>typology</i> of sound objects in the dataset. -->
        <!-- </p> -->

        <!-- <h2>Modeling The Sound Object's Typology</h2> -->
        <!-- <p> -->
        <!--     This is where the scattering network comes in. Lostanlen ... shows in many -->
        <!--     papers that scattering representations capture timbral similarities in complex -->
        <!--     sounds such as orchestral extended techniques. When reduced in dimensionality -->
        <!--     via an algorithm such as isomap, it can also be used to estimate the parameric -->
        <!--     span of FM, AM, and harmonic synthesizers. This capability ensures a bridge -->
        <!--     between typology and morphology, namely that "microstructural" control -->
        <!--     parameters follow the distribution of "mesostructural" representations. -->
        <!--     Considering our dataset of sound objects, we might consider that plotting our -->
        <!--     dataset in a similar isomap would yield a representation in 3D space that -->
        <!--     follows an unknown distribution of microstructural parameters. -->
        <!-- </p> -->
        <!-- <p> -->
        <!--     Therefore, a suitable question might not be which control parameters account -->
        <!--     for the least amount of correlation in the dataset, but rather which control -->
        <!--     parameters are <i>most correlated</i> with the dimensions of the JTFS isomap. -->
        <!--     Our experiment thus attempts to select the most disentangled control paramters -->
        <!--     based on which of our spectral audio descriptors are most correlated with with -->
        <!--     the isomap. -->
        <!-- </p> -->

        <!-- <h2>Experiments</h2> -->
        <!-- <p> -->
        <!--     With the motivation of exploring a more "acousmatic" situation for the neural -->
        <!--     audio model, we chose to run a number of experiments with three different -->
        <!--     audio datasets of friction percussion techniuqes, an extended technique that -->
        <!--     utilizes the scraping of nontraditional objects on a given drum head. We -->
        <!--     collected 20 - 30 recordings between 200ms and 1s of three different objects -->
        <!--     played on the membrane of a floor tom. These consisted of a spring coil (A), a -->
        <!--     threaded rod (B), and a small piece of styrofoam (C). -->
        <!-- </p> -->

        <!-- <h3>Acousmatic Control Parameters</h3> -->
        <!-- <p> -->
        <!--     Our first experiment involved training individual DDSP models on these -->
        <!--     datasets. The goal of this experiment was to explore ways in which -->
        <!--     disentangled control parameters can be extracted from "sound objects" in the -->
        <!--     dataset, and not from prior knowledge of sound in the dataset. The expected -->
        <!--     output of such a modification would result after training a DDSP model on the -->
        <!--     data, where the linear group actions of the control space would yield a more -->
        <!--     expressive output. -->
        <!-- </p> -->

        <!-- <p> -->
        <!--     We perform this experiment and train on the resulting control parameters for -->
        <!--     each dataset, comparing with the baseline control parameters of the given DDSP -->
        <!--     model. -->
        <!-- </p> -->

        <!-- <h3>Acousmatic Dataset Construction</h3> -->
        <!-- <p> -->
        <!--     An additional experiment further augments the Schaefferian circumstances of a -->
        <!--     DDSP model. In this experiment, we construct an aggregate dataset by taking -->
        <!--     the union of all three friction percussion datasets and performing the same -->
        <!--     experiment as above. This experiment applies a stricter condition of -->
        <!--     "acousmatic" since the notion of sound source is further abstracted. -->
        <!--     Evaluations for this condition are purely compositional, and consist of a -->
        <!--     number of timbre transfer examples. -->
        <!-- </p> -->

        <h2>Preliminary Models</h2>
        <!-- <p>Below are three models we recorded at CIRMMT.</p> -->
        <h3>Suspended Triangle</h3>
        <table>
            <thead>
                <tr>
                    <th>Sound Example</th>
                    <th>Description</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>
                        <audio controls>
                            <source
                                src="https://storage.googleapis.com/acousmatic-typomorphology-examples/tri-f.wav"
                                type="audio/mpeg"
                            />
                        </audio>
                    </td>
                    <td><b>Suspended Triangle</b> (Original Recording)</td>
                </tr>
                <tr>
                    <td>
                        <audio controls>
                            <source
                                src="https://storage.googleapis.com/acousmatic-typomorphology-examples/original.wav"
                                type="audio/mpeg"
                            />
                        </audio>
                    </td>
                    <td><b>Full DDSP Reconstruction</b></td>
                </tr>
                <tr>
                    <td>
                        <audio controls>
                            <source
                                src="https://storage.googleapis.com/acousmatic-typomorphology-examples/harm_one.wav"
                                type="audio/mpeg"
                            />
                        </audio>
                    </td>
                    <td>
                        <b>Extrapolated DDSP Reconstruction (Harmonic Energy = 1.0)</b>
                    </td>
                </tr>
                <tr>
                    <td>
                        <audio controls>
                            <source
                                src="https://storage.googleapis.com/acousmatic-typomorphology-examples/noise_one.wav"
                                type="audio/mpeg"
                            />
                        </audio>
                    </td>
                    <td><b>Extrapolated DDSP Reconstruction (Noise Energy = 1.0)</b></td>
                </tr>
            </tbody>
        </table>

        <h3>Noise Chirps</h3>
        <table>
            <thead>
                <tr>
                    <th>Sound Example</th>
                    <th>Description</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>
                        <audio controls>
                            <source
                                src="https://storage.googleapis.com/acousmatic-typomorphology-examples/Q1.wav"
                                type="audio/mpeg"
                            />
                        </audio>
                    </td>
                    <td><b>Noise Chirp (Q = 1.0)</b> (Original Recording)</td>
                </tr>
                <tr>
                    <td>
                        <audio controls>
                            <source
                                src="https://storage.googleapis.com/acousmatic-typomorphology-examples/reconstruction_Q1.wav"
                                type="audio/mpeg"
                            />
                        </audio>
                    </td>
                    <td><b>Full DDSP Reconstruction</b></td>
                </tr>
                <tr>
                    <td>
                        <audio controls>
                            <source
                                src="https://storage.googleapis.com/acousmatic-typomorphology-examples/kurtosis_0_Q1.wav"
                                type="audio/mpeg"
                            />
                        </audio>
                    </td>
                    <td><b>Extrapolated DDSP Reconstruction (Kurtosis = 0.0)</b></td>
                </tr>
                <tr>
                    <td>
                        <audio controls>
                            <source
                                src="https://storage.googleapis.com/acousmatic-typomorphology-examples/kurtosis_1_Q1.wav"
                                type="audio/mpeg"
                            />
                        </audio>
                    </td>
                    <td><b>Extrapolated DDSP Reconstruction (Kurtosis = 1.0)</b></td>
                </tr>
                <tr>
                    <td>
                        <audio controls>
                            <source
                                src="https://storage.googleapis.com/acousmatic-typomorphology-examples/decrease_0_Q1.wav"
                                type="audio/mpeg"
                            />
                        </audio>
                    </td>
                    <td><b>Extrapolated DDSP Reconstruction (Decrease = 0.0)</b></td>
                </tr>
                <tr>
                    <td>
                        <audio controls>
                            <source
                                src="https://storage.googleapis.com/acousmatic-typomorphology-examples/decrease_1_Q1.wav"
                                type="audio/mpeg"
                            />
                        </audio>
                    </td>
                    <td><b>Extrapolated DDSP Reconstruction (Decrease = 1.0)</b></td>
                </tr>
            </tbody>
        </table>

        <h2>Friction Percussion Models</h2>
        <!-- <p>Below are three models we recorded at CIRMMT.</p> -->
        <h3>Improvised Excerpts</h3>
        <table>
            <thead>
                <tr>
                    <th>Sound Example</th>
                    <th>Description</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>
                        <audio controls>
                            <source
                                src="https://storage.googleapis.com/acousmatic-typomorphology-examples/b-tt.mp3"
                                type="audio/mpeg"
                            />
                        </audio>
                    </td>
                    <td><b>Type A: </b>Tom with spring coil</td>
                </tr>
                <tr>
                    <td>
                        <audio controls>
                            <source
                                src="https://storage.googleapis.com/acousmatic-typomorphology-examples/d-tt.mp3"
                                type="audio/mpeg"
                            />
                        </audio>
                    </td>
                    <td><b>Type B: </b>Tom played with threaded rod</td>
                </tr>
                <tr>
                    <td>
                        <audio controls>
                            <source
                                src="https://storage.googleapis.com/acousmatic-typomorphology-examples/e-tt.mp3"
                                type="audio/mpeg"
                            />
                        </audio>
                    </td>
                    <td><b>Type C:</b>Tom played with miniature styrofoam</td>
                </tr>
            </tbody>
        </table>

        <h3>Reconstructions</h3>

        <div class="grid grid-3x3">
            <div>
                Type A (Original Recording)
                <audio controls>
                    <source
                        src="https://storage.googleapis.com/acousmatic-typomorphology-examples/b-original.mp3"
                        type="audio/mpeg"
                    />
                </audio>
            </div>
            <div>
                Baseline (DDSP Reconstruction)
                <audio controls>
                    <source
                        src="https://storage.googleapis.com/acousmatic-typomorphology-examples/b-baseline-output.mp3"
                        type="audio/mpeg"
                    />
                </audio>
            </div>
            <div>
                Disentangled (DDSP Reconstruction)
                <audio controls>
                    <source
                        src="https://storage.googleapis.com/acousmatic-typomorphology-examples/b-output.mp3"
                        type="audio/mpeg"
                    />
                </audio>
            </div>
        </div>
        <div class="grid grid-3x3">
            <div>
                Type B (Original Recording)
                <audio controls>
                    <source
                        src="https://storage.googleapis.com/acousmatic-typomorphology-examples/d-original.mp3"
                        type="audio/mpeg"
                    />
                </audio>
            </div>
            <div>
                Baseline (DDSP Reconstruction)
                <audio controls>
                    <source
                        src="https://storage.googleapis.com/acousmatic-typomorphology-examples/d-baseline-output.mp3"
                        type="audio/mpeg"
                    />
                </audio>
            </div>
            <div>
                Disentangled (DDSP Reconstruction)
                <audio controls>
                    <source
                        src="https://storage.googleapis.com/acousmatic-typomorphology-examples/d-output.mp3"
                        type="audio/mpeg"
                    />
                </audio>
            </div>
        </div>

        <div class="grid grid-3x3">
            <div>
                Type C (Original Recording)
                <audio controls>
                    <source
                        src="https://storage.googleapis.com/acousmatic-typomorphology-examples/e-original.mp3"
                        type="audio/mpeg"
                    />
                </audio>
            </div>
            <div>
                Baseline (DDSP Reconstruction)
                <audio controls>
                    <source
                        src="https://storage.googleapis.com/acousmatic-typomorphology-examples/e-baseline-output.mp3"
                        type="audio/mpeg"
                    />
                </audio>
            </div>
            <div>
                Disentangled (DDSP Reconstruction)
                <audio controls>
                    <source
                        src="https://storage.googleapis.com/acousmatic-typomorphology-examples/e-output.mp3"
                        type="audio/mpeg"
                    />
                </audio>
            </div>
        </div>

        <h2>Typomorphology</h2>

        <style>
            .table-wrapper {
                display: flex;
                justify-content: space-between; /* Adds space between the two tables */
                gap: 20px; /* Optional: add a gap between the tables */
            }
            .table-container {
                width: 45%; /* Adjust the width of each table container */
            }
        </style>

        <div class="table-wrapper">
            <div class="table-container">
                <h3>Dog Barking &rarr; Snare Drum</h3>
                <table border="1">
                    <tr>
                        <td>
                            Dog Barking (Original Recording)
                            <audio controls>
                                <source
                                    src="https://storage.googleapis.com/acousmatic-typomorphology-examples/og_dog.wav"
                                    type="audio/mpeg"
                                />
                                Your browser does not support the audio element.
                            </audio>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            Dog Barking (DDSP Reconstruction)
                            <audio controls>
                                <source
                                    src="https://storage.googleapis.com/acousmatic-typomorphology-examples/dog_reconstruction.wav"
                                    type="audio/mpeg"
                                />
                                Your browser does not support the audio element.
                            </audio>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            Snare Drum (Original Recording)
                            <audio controls>
                                <source
                                    src="https://storage.googleapis.com/acousmatic-typomorphology-examples/og_snare.wav"
                                    type="audio/mpeg"
                                />
                                Your browser does not support the audio element.
                            </audio>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            Snare Drum (DDSP Reconstruction)
                            <audio controls>
                                <source
                                    src="https://storage.googleapis.com/acousmatic-typomorphology-examples/snare_reconstruction.wav"
                                    type="audio/mpeg"
                                />
                                Your browser does not support the audio element.
                            </audio>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            Dog &rarr; Snare
                            <audio controls>
                                <source
                                    src="https://storage.googleapis.com/acousmatic-typomorphology-examples/dog_to_snare.wav"
                                    type="audio/mpeg"
                                />
                                Your browser does not support the audio element.
                            </audio>
                        </td>
                    </tr>
                </table>
            </div>

            <div class="table-container">
                <h3>Applause &rarr; Rainfall</h3>
                <table border="1">
                    <tr>
                        <td>
                            Applause (Original Recording)
                            <audio controls>
                                <source
                                    src="https://storage.googleapis.com/acousmatic-typomorphology-examples/og_clap.wav"
                                    type="audio/mpeg"
                                />
                                Your browser does not support the audio element.
                            </audio>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            Applause (DDSP Reconstruction)
                            <audio controls>
                                <source
                                    src="https://storage.googleapis.com/acousmatic-typomorphology-examples/clap_reconstructed.wav"
                                    type="audio/mpeg"
                                />
                                Your browser does not support the audio element.
                            </audio>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            Rainfall (Original Recording)
                            <audio controls>
                                <source
                                    src="https://storage.googleapis.com/acousmatic-typomorphology-examples/og_rain.wav"
                                    type="audio/mpeg"
                                />
                                Your browser does not support the audio element.
                            </audio>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            Rainfall (DDSP Reconstruction)
                            <audio controls>
                                <source
                                    src="https://storage.googleapis.com/acousmatic-typomorphology-examples/rain_reconstructed.wav"
                                    type="audio/mpeg"
                                />
                                Your browser does not support the audio element.
                            </audio>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            Applause &rarr; Rainfall
                            <audio controls>
                                <source
                                    src="https://storage.googleapis.com/acousmatic-typomorphology-examples/clap_to_rain.wav"
                                    type="audio/mpeg"
                                />
                                Your browser does not support the audio element.
                            </audio>
                        </td>
                    </tr>
                </table>
            </div>
        </div>

        <h2>Acknowledgements</h2>
        <pre>
        <code>
        @article{yourcitation,
            author = {Author Name},
            title = {Title of the Work},
            journal = {Journal Name},
            year = {Year},
        }
        </code>
    </pre>

        <h2>Citations</h2>
        <p>(IEEE citations)</p>
    </body>
</html>
